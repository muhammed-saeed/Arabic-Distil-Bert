{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ArabicDistelBert.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammed-saeed/Arabic-Distil-Bert/blob/main/ArabicDistelBert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3t5wx_8UV6q",
        "outputId": "c0f6a498-1862-49b6-a901-81a5b2f4ce00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Password for user â€˜msaeedâ€™: \n",
            "--2022-05-30 08:47:55--  https://oscar-prive.huma-num.fr/2109/packaged/ar/ar_part_1.txt.gz\n",
            "Resolving oscar-prive.huma-num.fr (oscar-prive.huma-num.fr)... 134.158.33.192\n",
            "Connecting to oscar-prive.huma-num.fr (oscar-prive.huma-num.fr)|134.158.33.192|:443... connected.\n",
            "HTTP request sent, awaiting response... 401 Unauthorized\n",
            "Authentication selected: Basic realm=\"Authentification LDAP\"\n",
            "Reusing existing connection to oscar-prive.huma-num.fr:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 289845638 (276M) [application/x-gzip]\n",
            "Saving to: â€˜ar_part_1.txt.gzâ€™\n",
            "\n",
            "ar_part_1.txt.gz    100%[===================>] 276.42M  25.5MB/s    in 11s     \n",
            "\n",
            "2022-05-30 08:48:08 (24.2 MB/s) - â€˜ar_part_1.txt.gzâ€™ saved [289845638/289845638]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --user=msaeed --ask-password https://oscar-prive.huma-num.fr/2109/packaged/ar/ar_part_1.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd ~\n",
        "!git clone https://github.com/huggingface/datasets.git\n",
        "%cd /content/datasets\n",
        "%pip install -e .\n",
        "%pwd\n",
        "%cd /content\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd /content/transformers\n",
        "%pip install .\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "V3GLmFq-U2w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d   \"/content/ar_part_1.txt.gz\""
      ],
      "metadata": {
        "id": "rFr9OxN8CB6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = \"\"\n",
        "with open(\"/content/ar_part_1.txt\", \"r\") as fb:\n",
        "  train_text = fb.readlines()\n",
        "\n",
        "with open(\"/content/train_file.txt\", \"w\") as fb:\n",
        "  fb.writelines(train_text[:1000])"
      ],
      "metadata": {
        "id": "Uvgqmp8DjLiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 /content/transformers/examples/xla_spawn.py --num_cores=8 transformers/examples/language-modeling/run_mlm.py --model_name_or_path init_distilbert --train_file 30gig_files/ar_file1.txt --do_train --do_eval  --output_dir output --validation_split_percentage=2 --per_device_eval_batch_size=2048 --num_train_epochs=1 --line_by_line"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLBQx28dVPh7",
        "outputId": "127fa421-1962-4b51-a393-7f0e7f53f087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/transformers/examples/xla_spawn.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/transformers/examples/pytorch/language-modeling/run_mlm.py --train_file /content/train_file.txt --model_type distilbert --tokenizer_name \"asafaya/bert-base-arabic\" --do_train --mlm_probability 0.0 --tpu_metrics_debug True --tpu_num_cores 8 --output_dir /content/test-mlm --overwrite_output_dir=True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSS4CViHKzr4",
        "outputId": "9db4365e-df85-406a-f2bc-cb2112eeaa93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:1090: FutureWarning: using `--tpu_metrics_debug` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--debug tpu_metrics_debug` instead\n",
            "  FutureWarning,\n",
            "05/30/2022 09:57:48 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "05/30/2022 09:57:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[<DebugOption.TPU_METRICS_DEBUG: 'tpu_metrics_debug'>],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/runs/May30_09-57-48_73cd6fa3e41c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/content/test-mlm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=8,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/30/2022 09:57:49 - WARNING - datasets.builder - Using custom data configuration default-64ae77cc493a92c2\n",
            "05/30/2022 09:57:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/30/2022 09:57:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:57:49 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/30/2022 09:57:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "100% 1/1 [00:00<00:00, 37.55it/s]\n",
            "05/30/2022 09:57:49 - WARNING - datasets.builder - Using custom data configuration default-64ae77cc493a92c2\n",
            "05/30/2022 09:57:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/30/2022 09:57:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:57:49 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/30/2022 09:57:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:57:49 - WARNING - datasets.builder - Using custom data configuration default-64ae77cc493a92c2\n",
            "05/30/2022 09:57:49 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/30/2022 09:57:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:57:49 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/30/2022 09:57:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:57:49 - WARNING - __main__ - You are instantiating a new config instance from scratch.\n",
            "[INFO|hub.py:583] 2022-05-30 09:57:49,738 >> https://huggingface.co/asafaya/bert-base-arabic/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6atcylnn\n",
            "Downloading: 100% 62.0/62.0 [00:00<00:00, 33.4kB/s]\n",
            "[INFO|hub.py:587] 2022-05-30 09:57:49,891 >> storing https://huggingface.co/asafaya/bert-base-arabic/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/0a21539d1796cc0b329be5eaa61ce3b2c419c62ae79908dd3e44eefdc9e11552.1788df22ba1a6817edb607a56efa931ee13ebad3b3500e58029a8f4e6d799a29\n",
            "[INFO|hub.py:595] 2022-05-30 09:57:49,891 >> creating metadata file for /root/.cache/huggingface/transformers/0a21539d1796cc0b329be5eaa61ce3b2c419c62ae79908dd3e44eefdc9e11552.1788df22ba1a6817edb607a56efa931ee13ebad3b3500e58029a8f4e6d799a29\n",
            "[INFO|hub.py:583] 2022-05-30 09:57:50,039 >> https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkcmdkvx2\n",
            "Downloading: 100% 491/491 [00:00<00:00, 256kB/s]\n",
            "[INFO|hub.py:587] 2022-05-30 09:57:50,188 >> storing https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|hub.py:595] 2022-05-30 09:57:50,188 >> creating metadata file for /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 09:57:50,189 >> loading configuration file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 09:57:50,190 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-base-arabic\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|hub.py:583] 2022-05-30 09:57:50,517 >> https://huggingface.co/asafaya/bert-base-arabic/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2o1pl72d\n",
            "Downloading: 100% 326k/326k [00:00<00:00, 2.91MB/s]\n",
            "[INFO|hub.py:587] 2022-05-30 09:57:50,812 >> storing https://huggingface.co/asafaya/bert-base-arabic/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/b826e2a08477c19ebf1b8b7dc4978f3f99a6f9d3107876282ea9691fafe37d98.63883f8c719f41c4f4feb08f611fd3bbe39009b245fef4ec83de8a16821590be\n",
            "[INFO|hub.py:595] 2022-05-30 09:57:50,813 >> creating metadata file for /root/.cache/huggingface/transformers/b826e2a08477c19ebf1b8b7dc4978f3f99a6f9d3107876282ea9691fafe37d98.63883f8c719f41c4f4feb08f611fd3bbe39009b245fef4ec83de8a16821590be\n",
            "[INFO|hub.py:583] 2022-05-30 09:57:51,343 >> https://huggingface.co/asafaya/bert-base-arabic/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp224vjnvn\n",
            "Downloading: 100% 112/112 [00:00<00:00, 38.6kB/s]\n",
            "[INFO|hub.py:587] 2022-05-30 09:57:51,497 >> storing https://huggingface.co/asafaya/bert-base-arabic/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/d9cecabc8bd0dab6bcb5e3905025e7b9b22dc8ee8ca848ef3e44843ee6599aa4.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|hub.py:595] 2022-05-30 09:57:51,498 >> creating metadata file for /root/.cache/huggingface/transformers/d9cecabc8bd0dab6bcb5e3905025e7b9b22dc8ee8ca848ef3e44843ee6599aa4.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:57:51,647 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b826e2a08477c19ebf1b8b7dc4978f3f99a6f9d3107876282ea9691fafe37d98.63883f8c719f41c4f4feb08f611fd3bbe39009b245fef4ec83de8a16821590be\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:57:51,647 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:57:51,647 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:57:51,647 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/d9cecabc8bd0dab6bcb5e3905025e7b9b22dc8ee8ca848ef3e44843ee6599aa4.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:57:51,648 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0a21539d1796cc0b329be5eaa61ce3b2c419c62ae79908dd3e44eefdc9e11552.1788df22ba1a6817edb607a56efa931ee13ebad3b3500e58029a8f4e6d799a29\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 09:57:51,797 >> loading configuration file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 09:57:51,798 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-base-arabic\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 09:57:52,019 >> loading configuration file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 09:57:52,021 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-base-arabic\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "05/30/2022 09:57:52 - INFO - __main__ - Training new model from scratch\n",
            "05/30/2022 09:57:55 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\n",
            "05/30/2022 09:57:55 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f108f57a9e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "05/30/2022 09:57:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-1c80317fa3b1799d.arrow\n",
            "05/30/2022 09:57:55 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f108f57aa70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "05/30/2022 09:57:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-bdd640fb06671ad1.arrow\n",
            "05/30/2022 09:57:55 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7f108f57a950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "05/30/2022 09:57:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-3eb13b9046685257.arrow\n",
            "05/30/2022 09:57:55 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7f108f57a9e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "05/30/2022 09:57:55 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-23b8c1e9392456de.arrow\n",
            "[INFO|trainer.py:628] 2022-05-30 09:57:55,141 >> The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1430] 2022-05-30 09:57:55,151 >> ***** Running training *****\n",
            "[INFO|trainer.py:1431] 2022-05-30 09:57:55,152 >>   Num examples = 191\n",
            "[INFO|trainer.py:1432] 2022-05-30 09:57:55,152 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1433] 2022-05-30 09:57:55,152 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1434] 2022-05-30 09:57:55,152 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1435] 2022-05-30 09:57:55,152 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1436] 2022-05-30 09:57:55,152 >>   Total optimization steps = 72\n",
            "  0% 0/72 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 608, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 557, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1327, in train\n",
            "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1565, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2245, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2277, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 656, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 566, in forward\n",
            "    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 129, in forward\n",
            "    word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
            "    self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2183, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "IndexError: index out of range in self\n",
            "  0% 0/72 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/transformers/examples/pytorch/language-modeling/run_mlm.py --train_file /content/train_file.txt --model_type distilbert --tokenizer_name \"asafaya/bert-base-arabic\"  --do_train --mlm_probability 0.0 --tpu_metrics_debug True --tpu_num_cores 8 --output_dir /content/test-mlm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdYt76zHBzkD",
        "outputId": "f76424a7-a7a4-41be-dc1f-d7ed89234ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/training_args.py:1090: FutureWarning: using `--tpu_metrics_debug` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--debug tpu_metrics_debug` instead\n",
            "  FutureWarning,\n",
            "05/30/2022 09:59:27 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "05/30/2022 09:59:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[<DebugOption.TPU_METRICS_DEBUG: 'tpu_metrics_debug'>],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/test-mlm/runs/May30_09-59-27_73cd6fa3e41c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=/content/test-mlm,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/test-mlm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=8,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/30/2022 09:59:27 - WARNING - datasets.builder - Using custom data configuration default-64ae77cc493a92c2\n",
            "05/30/2022 09:59:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/30/2022 09:59:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:59:27 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/30/2022 09:59:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "100% 1/1 [00:00<00:00, 486.35it/s]\n",
            "05/30/2022 09:59:27 - WARNING - datasets.builder - Using custom data configuration default-64ae77cc493a92c2\n",
            "05/30/2022 09:59:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/30/2022 09:59:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:59:27 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/30/2022 09:59:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:59:27 - WARNING - datasets.builder - Using custom data configuration default-64ae77cc493a92c2\n",
            "05/30/2022 09:59:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "05/30/2022 09:59:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:59:27 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n",
            "05/30/2022 09:59:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8\n",
            "05/30/2022 09:59:27 - WARNING - __main__ - You are instantiating a new config instance from scratch.\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 09:59:28,092 >> loading configuration file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 09:59:28,094 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-base-arabic\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:59:28,991 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/b826e2a08477c19ebf1b8b7dc4978f3f99a6f9d3107876282ea9691fafe37d98.63883f8c719f41c4f4feb08f611fd3bbe39009b245fef4ec83de8a16821590be\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:59:28,991 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:59:28,991 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:59:28,991 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/d9cecabc8bd0dab6bcb5e3905025e7b9b22dc8ee8ca848ef3e44843ee6599aa4.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1781] 2022-05-30 09:59:28,991 >> loading file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0a21539d1796cc0b329be5eaa61ce3b2c419c62ae79908dd3e44eefdc9e11552.1788df22ba1a6817edb607a56efa931ee13ebad3b3500e58029a8f4e6d799a29\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 09:59:29,139 >> loading configuration file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 09:59:29,140 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-base-arabic\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 09:59:29,335 >> loading configuration file https://huggingface.co/asafaya/bert-base-arabic/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9c5e190e76ab27038700323eb24fad10a0ad90017d4d8a5293e1a2d361823226.9be42b5fcc148338c3790d2adb94f47c610eed8c4dda383d092c3afea817185e\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 09:59:29,336 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"asafaya/bert-base-arabic\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.20.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "05/30/2022 09:59:29 - INFO - __main__ - Training new model from scratch\n",
            "05/30/2022 09:59:31 - WARNING - __main__ - The tokenizer picked seems to have a very large `model_max_length` (1000000000000000019884624838656). Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\n",
            "05/30/2022 09:59:31 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f9342ee0950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "05/30/2022 09:59:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-1c80317fa3b1799d.arrow\n",
            "05/30/2022 09:59:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f9342ee09e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "05/30/2022 09:59:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-bdd640fb06671ad1.arrow\n",
            "05/30/2022 09:59:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7f9342ee08c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "05/30/2022 09:59:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-3eb13b9046685257.arrow\n",
            "05/30/2022 09:59:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7f9342ee0950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "05/30/2022 09:59:32 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-64ae77cc493a92c2/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8/cache-23b8c1e9392456de.arrow\n",
            "[INFO|trainer.py:628] 2022-05-30 09:59:32,038 >> The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1430] 2022-05-30 09:59:32,045 >> ***** Running training *****\n",
            "[INFO|trainer.py:1431] 2022-05-30 09:59:32,046 >>   Num examples = 191\n",
            "[INFO|trainer.py:1432] 2022-05-30 09:59:32,046 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1433] 2022-05-30 09:59:32,046 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1434] 2022-05-30 09:59:32,046 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1435] 2022-05-30 09:59:32,046 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1436] 2022-05-30 09:59:32,046 >>   Total optimization steps = 72\n",
            "  0% 0/72 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 608, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/language-modeling/run_mlm.py\", line 557, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1327, in train\n",
            "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1565, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2245, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2277, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 656, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 566, in forward\n",
            "    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\", line 129, in forward\n",
            "    word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\", line 160, in forward\n",
            "    self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 2183, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "IndexError: index out of range in self\n",
            "  0% 0/72 [00:00<?, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 /content/transformers/examples/pytorch/language-modeling/run_mlm.py --train_file output.txt --model_type distilbert --tokenizer_name distilbert-base-uncased --do_train --mlm_probability 0.0 --tpu_metrics_debug True --tpu_num_cores 8 --output_dir /content/test-mlm"
      ],
      "metadata": {
        "id": "J4ftKz1-WsW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf \"/content/test-mlm\""
      ],
      "metadata": {
        "id": "3Raxq3urYEmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L_mfHm-6kbiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}