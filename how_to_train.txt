python3 transformers/examples/xla_spawn.py --num_cores=8 \
   transformers/examples/language-modeling/run_mlm.py \
    --model_name_or_path init_distilbert\
    --train_file 30gig_files/ar_file1.txt \
    --do_train \
    --do_eval  \
    --output_dir output \
    --validation_split_percentage=2 \
    --per_device_eval_batch_size=2048 \
    --num_train_epochs=1 \
    --line_by_line


///////////////////////////////////////////////////////
the above code was used in the old days,, however now we are using a more updated version of the code
"however this code uses distilbert-base-uncased vocabulary which is english one"


python3 transformers/examples/pytorch/language-modeling/run_mlm.py --train_file output.txt \
 --model_type distilbert \ 
 --tokenizer_name distilbert-base-uncased \
 --do_train \
 --mlm_probability 0.0 \
 --tpu_metrics_debug True \
 --tpu_num_cores 8 \
 --output_dir /home/data_driven_project_enigma/tmp_bert/test-mlm

\\\\\\\\\\\\\\\\\\\\\/////////////////////

python3 transformers/examples/pytorch/language-modeling/run_mlm.py   --train_file output.txt --model_type distilbert  --tokenizer_name distilbert-base-uncased   --tpu_metrics_debug True --tpu_num_cores 8  --do_train  --mlm_probability 0.0   --output_dir /home/data_driven_project_enigma/tmp_bert/test-mlm



////////////////////\\\\\\\\\\\\\\\\/////////////////////////\

$$$$$ pre-training our distilbert arabic model_name_or_path

## Using CPU only (without TPU)
python3 transformers/examples/pytorch/language-modeling/run_mlm.py \
    --line_by_line \
    --config_name /home/data_driven_project_enigma/tester/ \
    --tokenizer_name /home/data_driven_project_enigma/tester/ \
    --train_file /home/data_driven_project_enigma/output.txt \
    --max_seq_length 512 \
    --do_train \
    --output_dir /home/data_driven_project_enigma/mine_vocab_method/test-mlm
####
#using TPU
python3 transformers/examples/pytorch/language-modeling/run_mlm.py \
    --line_by_line \    
    --config_name /home/data_driven_project_enigma/tester/ \
    --tokenizer_name /home/data_driven_project_enigma/tester/ \
    --train_file /home/data_driven_project_enigma/output.txt \
    --max_seq_length 512 \
    --tpu_metrics_debug True \
    --tpu_num_cores 8 \
    --do_train \
    --output_dir /home/data_driven_project_enigma/mine_vocab_method/test-mlm